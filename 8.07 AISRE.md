## 一、业务理解与场景认知
- 目标：深刻理解AI在短视频业务中的资源消耗模式（如：模型推理/训练的GPU使用，视频渲染的存储IO压力等）。

1. 资源类型多样，调度复杂（实时服务（如推荐） vs 批处理任务（如离线训练）混布），不同团队、不同区域对资源的需求时间错开或重叠。模型大小不一、GPU资源需求不均；

2. 资源利用率低，成本高：GPU空置率高，业务高峰期抢占资源严重，多模型部署导致重复存储和浪费

3. 资源分配与成本归因难：很多资源为共享集群模式，难以归因到具体业务，需要按团队/项目/功能模块细分使用量和费用，

a. GPU GPU 是 图形处理器（Graphics Processing Unit），本质上是一种专门用于并行计算的处理器
。
| 对比项  | CPU（中央处理器）                | GPU（图形处理器）                     |
| ---- | ------------------------- | ------------------------------ |
| 定位   | 通用计算，擅长逻辑处理、流程控制          | 大规模并行计算，擅长处理重复计算任务             |
| 核心数量 | 少（2\~64核）但每个核很强           | 多（上千个小核）但单核能力较弱                |
| 适用场景 | 操作系统运行、后台逻辑控制、IO操作        | AI训练/推理、图像渲染、视频解码              |
| 执行方式 | 串行为主 + 少量并发               | 完全并行化                          |
| 内存结构 | 大缓存、层级复杂                  | 高带宽、共享显存                       |
| 举例   | Intel i9, AMD Ryzen, Xeon | NVIDIA A100, H100, T4, RTX4090 |

GPU 运算流程：
你的 AI 模型在使用 GPU 时，实际上是框架通过 CUDA 发出一系列指令，GPU 执行完并把结果返回。

📦 场景一：模型训练
模型训练本质是通过不断前向传播（推理）和反向传播（梯度计算）来更新参数。

这些过程需要大量 矩阵运算，比如 A × B、卷积等。

所以训练任务使用 GPU 时，是把整个 batch 的数据和模型参数 load 到 GPU 显存 中，然后调用 CUDA 核函数执行训练逻辑。

🚀 场景二：模型推理（Inference）
模型已经训练好，输入数据 → 模型 → 输出结果

推理只涉及前向传播，一般计算量小于训练，但要求 低延迟。

推理服务常常部署成 REST API，接收到请求时，将输入转成 tensor → 放入 GPU → 推理 → 拿到输出 → 返回响应。

1. GPU 分配机制：
在部署 Pod 时，声明需要 nvidia.com/gpu: 1；K8s 调度器会将这个 Pod 调度到有空闲 GPU 的节点；NVIDIA Device Plugin 负责把该 GPU 设备挂载进容器；容器内的模型代码才能通过 cuda:0 访问 GPU。

2. 多 GPU 的管理：节点上可以有 1~8 张 GPU；每张 GPU 的使用情况由 NVIDIA 驱动统一管理；可通过 MIG 技术把一张 GPU 分成多个隔离的子 GPU（适合小模型并发跑）。

b. 视频存储IO瓶颈通常体现在哪？

上传/转码阶段：原始素材 + 中间文件 + 转码副本，占用大量空间;突发写入带宽压力大

AI处理阶段: 需要从存储中频繁读入视频帧、图像片段，带来 随机读压力; 多个任务并发读取，容易引起 IO拥塞 / NFS卡顿

解决方案建议：使用并行文件系统或对象存储（如MinIO、OSS）；多级缓存（如 SSD cache + 冷存储分层）拆分长视频为小块进行并发处理；引入 async prefetch（异步预取）缓解 IO 问题。

c. GPU 资源需求不均怎么办？

优化策略：显存隔离 + GPU 多租户(利用 MIG（Multi-Instance GPU） 技术划分 GPU 实例),实现不同任务之间的物理隔离但共享计算资源;

按需调度 + 限制抢占:K8s 配置资源亲和性，按显存需求调度到合适节点; 

将不同需求分层管理: 高显存模型统一调度到专用 GPU 节点; 推理服务集中调度在支持 GPU 共享的节点上

d. GPU 空置率高怎么办？

原因可能包括：任务量低、非持续调用;资源绑死（一个推理服务独占一个GPU）;调度粒度粗（K8s不支持 GPU 共享或复用）

解决方案：服务合并部署,将多个小模型服务封装为一个 Pod; GPU 共享与弹性调度,非高优先级任务使用空闲GPU进行“抢占式”调度; 训练任务调度到业务低峰（如夜间），提升整体利用率; 建立 GPU 使用预留机制，闲置自动回收


- 能力要求：

对生成式AI和短视频平台架构有了解（如：AIGC、Diffusion Model、LLM、视频转码流程）。

能识别不同业务场景对资源（GPU/CPU/内存/存储）的负载特性。
| 生成式 AI 应用       | 用途              | 消耗资源                 |
| --------------- | --------------- | -------------------- |
| 文本生成（LLM）Transformer       | 自动写文案、评论生成、标题优化GPT-4 | CPU负载低/GPU，Token计算成本高   |
| 语音合成（TTS）       | 给视频配音，多语言语音生成   | GPU，低延迟要求高           |
| 图像生成（Diffusion） | Stable Diffusion、DALL·E自动生成封面、背景、贴纸    | GPU显存占用大、耗时长           |
| 视频转码流程         | 将原始视频（高码率、高分辨率）转换为适合不同设备和平台的视频格式，如 H.264 → H.265，或压缩分辨率   | CPU + 存储IO密集型        |
| 智能剪辑            | 提取高光片段、视频摘要     | CPU/GPU混合，需高吞吐       |
| 多语言翻译/字幕        | 实时字幕、多语种分发      | GPU推理（如NLLB、Whisper） |
| 内容理解（Embedding） | 视频向量化、标签推荐，视频推荐，RAG      | GPU矩阵密集 + 存储，Batch任务适合离线任务     |

| 技术/模型     | 使用资源         | 特点说明                 |
| --------- | ------------ | -------------------- |
| AIGC      | GPU + 存储     | 任务种类多，负载分布不均         |
| Diffusion | 高端GPU + 显存   | 图像生成慢，显存要求高          |
| LLM       | GPU推理 + 网络   | Token越多越吃资源，多用户并发需调度 |
| Embedding | GPU + 向量数据库  | 批处理友好，适合离线计算         |
| ASR       | GPU推理 + IO   | 音频长时延迟大，可分片优化        |
| 视频转码      | CPU/GPU + IO | 带宽和磁盘瓶颈明显，适合流水线优化    |
推理模型（如 Chat 模型）：低延迟、高并发，对显存要求低但 GPU 利用率差，倾向于部署在共享 GPU 池中

训练模型（如 Diffusion / LLM 微调）：显存需求大，长时间占用，需要独占高性能 GPU（如 A100）

Embedding 计算：适合使用中性能 GPU，调度到空闲时间段批量跑

图像/视频模型：需要高 IO + 存储缓存，我会规划高速 SSD + 并发对象存储分层架构

模型体积大的，会使用持久化挂载点提前缓存模型，加速调度

AIGC 指的是通过 AI 模型自动生成的内容，如文本、图像、语音、视频等。

ASR（Automatic Speech Recognition）：语音转文字技术（多语言识别，语音转字幕），可做批量处理，但语音时长长，推理时间长，	对 GPU 延迟要求极高

Transformer 的核心理念是：用 Attention（注意力机制），实现并行处理。我们通常只用 Decoder（自回归方式）来逐字生成内容。
<img width="589" height="311" alt="image" src="https://github.com/user-attachments/assets/1518c0f1-5fc7-432f-a7aa-eda657b608e7" />

| 模块                                  | 作用                                      |
| ----------------------------------- | --------------------------------------- |
| **Self-Attention**                  | 每个词都能“看到”输入序列中的所有词，通过加权判断哪些词更重要（这就是注意力） |
| **Multi-Head Attention**            | 使用多个注意力机制（头）来捕捉不同的关系特征                  |
| **Position Encoding**               | 因为模型是并行的，没有顺序概念，所以加上“位置编码”来保留词语顺序信息     |
| **Feed Forward Layer**              | 每个词向量都通过一个全连接网络进行处理                     |
| **Residual Connection + LayerNorm** | 稳定训练过程，加速收敛，提高泛化能力                      |

Batch（批次），是指在模型训练或推理时，一次性送入神经网络进行计算的数据样本数量。
| 用途              | 说明                                             |
| --------------- | ---------------------------------------------- |
| **提升 GPU 并行效率** | GPU 擅长处理矩阵运算，把多个样本一起变成大矩阵，可以同时处理，利用 GPU 并行能力 ✅ |
| **加快训练速度**      | 比逐个样本处理快得多。Batch 越大，一次计算处理的样本越多，整体训练越快 ✅       |
| **梯度更稳定**       | 在训练中使用 Batch，可以将多个样本的梯度平均，减少波动，提高模型稳定性 ✅       |
| **内存可控**        | Batch Size 可调节，用小的 Batch Size 就不会 OOM（爆显存） ✅   |
| **便于工程并行**      | 分布式训练、推理服务时，Batch 是划分计算任务的重要单位 ✅               |

梯度就是模型“预测错了以后，该怎么调整参数”的方向和幅度。实际上我们会 沿着梯度的相反方向 调整参数 —— 这就是“梯度下降法（Gradient Descent）“，我们的目标是 最小化损失函数

RAG（Retrieval-Augmented Generation）= 检索 + 生成，它是一种 将外部知识库与大语言模型结合 的架构，用于解决 LLM 的知识盲点问题。训练数据有时效性，训练集有限，是一种通过结合外部知识库与大语言模型来增强回答准确性的方法。它先从知识库中检索出与用户问题相关的内容，再将这些内容与用户问题一起送入 LLM 做生成。
| 步骤               | 说明                                                  |
| ---------------- | --------------------------------------------------- |
| 1️⃣ 数据预处理        | 将 PDF、Word、网页、FAQ 拆分成小段（chunk），并进行嵌入向量化             |
| 2️⃣ 向量存储         | 将所有文档向量存入 **向量数据库**（如 FAISS、Milvus、Azure AI Search） |
| 3️⃣ 检索 Retriever | 用户提问 → 向量化 → 在向量库中查找相似内容（Top-K）                     |
| 4️⃣ Prompt 构造    | 把问题 + 检索内容拼接成 prompt，交给 LLM（如 GPT-4）                |
| 5️⃣ LLM 生成       | 基于上下文生成更真实、有据可依的回答                                  |


❓Q1：你是如何理解生成式AI在短视频业务中对资源的消耗模式的？
参考回答：
生成式AI在短视频中常用于字幕生成、语音合成、图像/视频生成等场景。
这些任务大多依赖高算力GPU资源，尤其是推理服务的并发处理能力。

例如：语音转文字（ASR）多为实时调用，对低延迟和稳定GPU分配有要求；

图像生成类任务则需要高吞吐、批量调度；

此外，存储层面对中间模型、训练数据集也要求较高的I/O性能与空间优化。

二、资源与成本管理
❓Q2：请介绍一个你做资源利用率监控或优化的案例？

资源归因（Resource Attribution）就是把一台机器、一块GPU、一块磁盘所消耗的资源、费用，精准地“算账”到具体的业务、团队、模型或项目上。
| 归因对象    | 举例                         |
| ------- | -------------------------- |
| 业务线     | 视频推荐、直播、广告投放               |
| 项目 / 模型 | LLM-Chat、图像生成 Diffusion-v3 |
| 用户 / 团队 | 内部账号、AI Infra团队、内容中心       |
| 部署维度    | 某命名空间、某 GPU 队列、某 Region    |
方案 1：Kubernetes Label + Metrics 归因（给pod打标签然后进行可视化再聚合）sum(rate(container_gpu_utilization{project="chatgpt-serving"}[5m]))
方案 2：作业系统内归因（Job-based Accounting）（适用于模型训练 / 离线任务场景）任务调度系统记录执行时间、用的资源节点，job_4567 使用 GPU 8h等
方案 3：账单归因（Billing-based）（适用于云上环境）云资源创建时绑定标签（resource group, cost center, environment），使用 Billing API 获取每个标签组合的月度账单

❓Q3：你如何进行资源成本归因和预算控制？
参考回答：
我采用标签打标 + 成本聚合方式实现资源归因：

在 Kubernetes 层对资源打上 team/project label；

结合云厂商的账单API，将消耗的CPU/GPU/存储归因到具体项目；

利用自研预算管理平台 + PowerBI，每月汇总资源成本，并发送邮件报告；

设置阈值告警，例如：资源使用或预算超出80%提前通知业务方。

三、容量规划与资源保障
❓Q4：如何制定一场全球活动（如短视频挑战赛）期间的资源保障方案？
参考回答：
我会从以下几个方面进行保障：

预测模型：参考历史活动 PV/UV、模型调用次数，使用时间序列预测工具（如 Prophet）预测峰值负载；
PV（Page View）：页面浏览量，表示网页或功能页被访问的总次数。
UV（Unique Visitor）：独立访客数，表示有多少唯一用户访问了页面或系统。

资源预留：提前在 GPU 资源池中做容量预留或借用策略（借调其他业务资源）；

分批扩容：基于活动时间分段分批扩容，采用 HPA/VPA 自动缩放；

故障预案：制定灾备切流策略，例如从主区域自动切换至备用区域；

监控看板：搭建 Grafana dashboard 实时追踪关键资源指标（GPU、存储IO、网络延迟等）。

四、资源监控与平台自动化
❓Q5：你会如何搭建一个资源监控系统，用于追踪 GPU/CPU/存储使用情况？
参考回答：
我通常采用以下技术方案：

使用 Prometheus + Node Exporter 收集 CPU/内存/磁盘 等基本资源；

对于 GPU，使用 DCGM-Exporter 采集 GPU 温度、功耗、使用率、显存等指标；

数据统一汇入 Prometheus，展示到 Grafana；

实现多维度查询（按业务、团队、服务维度聚合）；

对接 Alertmanager 设定告警规则，异常资源使用时发送飞书/Slack告警。

❓Q6：如何将上述管理方案落地为自动化平台工具？

五、商品定价与资源售卖设计
❓Q7：你如何设计一个资源服务（如GPU训练服务）的内部售卖和定价机制？
参考回答：
我会从以下几个维度考虑定价：

资源成本：包括云GPU单价、运维成本、折旧成本；

资源类型：区分高性能GPU（A100）与低端GPU（T4）的定价；

使用方式：提供按时计费、包月套餐、按次任务等不同售卖模型；

服务等级：根据 SLA 提供不同等级（如经济型、标准型、高可用型）；
最终设计成内部计费商品体系，并通过资源平台或API面向业务方售卖。

六、相关技术补充
❓Q8：请解释 Kubernetes 中如何进行资源配额与管理？
参考回答：

K8s 中使用 ResourceQuota 和 LimitRange 控制资源使用：ResourceQuota：限制命名空间级别的资源总量（如最多使用10个CPU）；LimitRange：控制 Pod/container 的最小/最大资源请求值；
结合 HPA/VPA 实现自动扩容；此外，可使用自定义 controller 或插件，对 GPU 使用率进行更精细的调度（如基于节点负载选Pod落点）。

❓Q9：你了解 FinOps 吗？如何在企业中推动 FinOps 实践？
参考回答：
| 阶段               | 目标               | 举例                   |
| ---------------- | ---------------- | -------------------- |
| **Inform（信息化）**  | 提供清晰的资源使用和费用视图   | 建立成本仪表盘，按团队/项目归因     |
| **Optimize（优化）** | 查找浪费和优化机会，提升资源效率 | 弹性伸缩、关停空闲实例、使用预留资源   |
| **Operate（运营）**  | 建立机制持续改进，形成闭环治理  | 建立预算、责任人制度、月度对账和优化会议 |

FinOps 是云成本优化的最佳实践体系，强调“协同透明的成本管理”。FinOps = Financial + DevOps，是一种跨部门协作的 云成本管理与优化方法论
推动 FinOps 可从以下几点着手：

建立成本中心，明确谁对成本负责；按 team / project / service 维度进行资源打标签；聚合使用量生成周报日报

提供透明账单与成本归因平台；与财务协作，按部门/项目分配年度预算；

落地预算控制、成本预警、节省推荐；研发项目负责人要知道自己服务用了多少资源、花了多少钱

建立成本周会机制，推动研发/运维/财务共同参与优化；建立“节省建议处理流程”：如使用 Azure Advisor 的推荐；引入工具如 CloudHealth、Kubecost、Spot.io 辅助治理。

- 你做过容量规划吗？是基于什么预测的？

我主要依据以下几方面做预测：

1. 业务增长趋势（历史 QPS、模型调用次数、活跃用户量等）
| 指标                           | 说明                        |
| ---------------------------- | ------------------------- |
| **QPS (Queries Per Second)** | 每秒请求量，反映用户访问压力            |
| **模型调用次数**                   | 每天/每小时推理模型的总次数，影响 GPU 使用量 |
| **活跃用户数（DAU/MAU）**           | 用户越多，请求增长越快，资源需求越高        |
| **视频上传/转码量**                 | 短视频平台独有，涉及存储+转码算力需求       |
| **历史数据增长率**                  | 例如同比增长 30%，可作为预测因子        |

预测未来 1个月、1季度资源是否会超出现有能力，决定是否需要新增服务器 / 扩容 GPU 池

2. 服务本身的单位资源消耗（如每次模型推理平均使用 0.5s GPU 时间 + 2GB 显存）：总QPS × 单次消耗 = 总资源需求

3. 历史活动负载，结合时间序列预测模型（如 Prophet）模拟高峰期增长：在大促、节假日、挑战赛等活动中，流量会突然暴涨。要预测这种“非线性”增长。
收集历史活动当天的负载数据（比如去年618、黑五）

对比普通日常负载，得出增幅（如 3倍 QPS）

使用时间序列预测模型（如 Prophet）来拟合增长趋势，预测今年流量峰值和时间分布；精准规划活动所需的额外 GPU / 存储 / 网络资源；提前准备热节点或弹性扩容策略

4. Buffer 策略：一般会加入 20~30% 冗余容量应对突发流量，加入 20~30% 的 buffer 作为资源预留；实现负载波峰时不崩溃、不掉服务；也适合容错，比如主节点挂了能自动 failover


- 如何防止服务“抢资源”或“不够用”？

在 Kubernetes 层 设置 ResourceQuota 和 LimitRange 控制命名空间内最大资源使用

引入 队列调度机制（如 Volcano、Yunikorn）对任务设置优先级和配额

推理服务使用 GPU Pool 共享，避免每个服务独占 GPU

使用 Prometheus + Grafana 实时监控 GPU 使用，触发告警或预扩容

加入熔断机制：当服务使用超过阈值时自动限流或降级

-  9. 你怎么制定资源调度和兜底预案？
答：
我会分三层制定保障机制：

事前准备：负载预测 + 容量评估 + GPU预热 + 节点弹性准备；SLA 分级（核心服务90%以上资源优先）

活动中调度策略：实时监控 GPU/模型/接口RT/QPS；使用自研调度器 + 优先级队列处理模型调用

事后兜底：出问题自动降级切灰（如从LLM切回规则模型）；多 Region 容灾自动切换；GPU 服务失败 fallback 到 CPU 服务版本

最终实现了业务“0中断、0事故、<200ms响应”的保障目标。
