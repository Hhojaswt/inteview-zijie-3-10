## 1. 你如何做资源需求预测？用过哪些方法和数据指标？

- 历史预测：从 历史数据分析（CPU/GPU 使用率、存储增长曲线）按月/季度的资源使用曲线，包含季节性节日性业务波动，找出增长率和季节性波动。 → 业务增长预测 → 弹性冗余策略 三步展开。
业务驱动预测：根据销售 pipeline、大客户合同、即将上线的大项目 → 转换成资源需求。AI/视频/大模型业务通常会提前报需求，可以作为预测依据。

- 容量规划模型：设定资源冗余阈值（比如 CPU ≥ 60% 利用率就触发扩容）。结合 SLA 和库存周转周期，反推需要提前多少资源。按目标服务等级设定缓冲（例如在线 P95 流量+5–15%，大促前+20–30%）

- 可视化与监控：Grafana + Prometheus（实时监控，绘制曲线）Azure Monitor / Log Analytics / Metrics Explorer，火山引擎内部可能有类似 BI/监控平台。
提及用过的工具/方法：Prometheus + Grafana、内部报表、回归预测模型。
数据分析：Excel / PowerBI（简单趋势+增长率预测）；SQL（分析历史使用数据、分组聚合）；Python（Pandas + statsmodels/Prophet）做更复杂的时间序列预测。

- 目的是什么：E（Efficiency）效率：提升资源利用率（CPU/内存/GPU/带宽）与调度吞吐。

- C（Cost）成本：压缩单位算力成本（单核/单GB/单TFLOPS），提升毛利。

- R（Reliability）可靠性：确保SLO/SLA，关键路径冗余与故障域隔离。
权衡话术：在线业务优先R，其次C与E；离线/AI训练优先E和C；内部多租户以公平性+隔离性为底线，用优先级和预算约束做动态折中。

## 2. 如果一个客户临时要 1000 块 GPU，但当前池子只有 600 块，你会如何处理？

- 短期措施：优先级调度：区分客户级别（大客户 / 战略客户 / 普通客户），保证高优先级项目。

跨区域调用：看其他 Region 有没有富余 GPU，通过跨区调度补足。

临时采购/租赁：联系合作伙伴或 OEM 资源池，做短期补充。

- 中期措施：协调供应链：加急订购，推动 GPU 现货交付。

项目切分：和客户沟通是否可以分批交付（先交付 600，用作训练 warm-up，剩余批次再补）。

- 长期措施：预测模型更新：把这次突发需求纳入预测因子，避免低估未来 GPU 需求。

大客户提前锁定机制：签合同时要求客户报备 GPU 用量，做资源预留。

跨业务优先级机制：定义内部调度策略（大模型训练 > 普通推理 > 测试任务）。 

## 3.1 公有云 IaaS 产品中，GPU 和 CPU 的供需管理差异在哪？

第一，供给差异：CPU 产能充足、交付周期短；GPU 产能有限，特别是高端型号，供应链周期长。

第二，调度差异：CPU 可以过量分配，调度灵活；GPU 很多任务需要独占，利用率提升更依赖虚拟化或 MIG。

第三，需求差异：CPU 需求相对平稳；GPU 需求波动大，经常因为 AI 或大模型项目出现突发式需求。

第四，成本与风险差异：CPU 成本低、库存风险小；GPU 成本高昂，既怕供给不足影响交付，也怕囤货闲置。

## 3.2 如何提升GPU的利用率
- 技术层面，可以通过 GPU 虚拟化（如 MIG、vGPU）来拆分 GPU、精细化分配显存；通过 Kubernetes 等调度器支持时间片共享和优先级调度，避免低优先任务长期占用资源；同时结合CPU + GPU 协同：一些轻量任务放到 CPU/GPU 混合架构执行。FPGA/ASIC 替代：针对特定场景（推理/视频转码）用更高效硬件，减少 GPU 占用。，让 GPU 专注在最需要的任务上。

📌 一、Kubernetes + GPU 调度器（Volcano / KubeFlow）

Kubernetes 本身：原生只支持给 Pod 分配“整块 GPU”，即 nvidia.com/gpu: 1 这样的方式。

Volcano / KubeFlow 等调度器：在 K8s 上扩展了 Job 级别调度，支持：

Gang Scheduling（一组任务必须同时分配到 GPU，否则不运行，避免浪费）

队列化调度（多个任务排队，按优先级分配 GPU）；配额和公平性（不同用户/项目公平使用 GPU）

📌 二、时间片 / 分时共享（Time-slicing）

原理：通过 NVIDIA Device Plugin + CUDA MPS（Multi-Process Service） 或者 MIG，把一块 GPU 分时分配给多个进程。

- 实现方式：MPS 模式：多个进程可以同时共享 GPU，一个任务不用独占整个 GPU 卡。

时间片调度：NVIDIA 驱动会在底层切换任务（类似 CPU 上的多进程调度）。

在 Kubernetes 里：部署 NVIDIA Device Plugin，配置 time-slicing 策略。

📌 三、抢占式 GPU 实例（Preemptible / Spot GPU）

原理：和 CPU Spot VM 类似，GPU 也可以作为 抢占式资源。

- 实现方式：Kubernetes/调度器设置 优先级队列。

当高优先级任务来时，可以强制中断低优先级任务，释放 GPU。

在公有云（GCP/Azure/AWS/火山引擎）中，提供 抢占式 GPU 实例，价格更低，但随时可能被回收。

Pod 可以申请“部分 GPU”，比如 nvidia.com/gpu: 0.25，实际上是被分配到时间片上的。

📌 四、GPU 是否可以扩缩容？

CPU/内存：可以直接水平扩缩容（比如增加 Pod，调度更多 vCPU）。

GPU：硬件资源有限，不像 CPU 可以“超分配”。

GPU 扩缩容的实现方式：集群级扩缩容（Cluster Autoscaler）如果集群里 GPU 节点不够，K8s 可以调用底层 IaaS（比如 Azure VMSS、AWS ASG）去创建新的 GPU 节点。这相当于 横向扩容 GPU。

虚拟化/MIG 划分：一块 GPU 被切成多个子实例，按需分配 → 相当于 纵向切分，但不是无限扩容。

分时共享：通过时间片方式，把 GPU 拆成更多逻辑“份额”，实现“软扩容”。

- 运营层面，可以通过客户引导、分时计费来减少闲置，引导客户使用 按需 / 竞价实例，避免 GPU 长时间空置。推广 分时计费，鼓励客户按小时甚至分钟租用，而不是长期占用。；建立 GPU 资源池和调度平台，做自动化伸缩；大客户预留池：提前锁定资源，避免盲目扩容。优先级队列：SLA 不同的客户进入不同优先级队列，减少资源浪费。利用率监控与回收；同时用监控回收空闲资源，设置 Idle Timeout，提高整体池的利用率。

这样既能在技术上减少碎片浪费，又能在业务上提升 GPU 投入产出比。

## 4. 你在云厂商做过的资源运营优化案例？

举例：通过 Spot/竞价实例、储蓄计划、预留实例 来优化成本。

通过 资源回收、闲置实例压缩 提高利用率。

展现对 成本 vs SLA 的平衡 有实操经验。


## 5. 你支持过的最大项目是什么？

说明项目背景（比如：AI 大模型训练客户，需要数千块 GPU）。

你的职责（资源方案设计、供给协调、交付保障）。

难点（资源紧缺、交付时间紧）。

结果（SLA 达标、成本优化 XX%）。

## 6. 资源不足时，你是如何保障关键客户的？

描述一次真实经历（比如云厂商 GPU 突然告急）。

你的措施：优先级排序、跨区调度、借用合作伙伴机房。

监控与预警：我基于 Azure Container Insights + AMP，监控 GPU 节点利用率和关键 API 延迟，当利用率超过 90% 持续 5 分钟即触发告警。

优先级调度：为关键 Pod 设置更高的 QoS（Quality of Service）等级，并结合 HPA（水平 Pod 自动伸缩器），保证风控引擎和支付网关能优先扩容。

资源腾挪：通过 Prometheus 指标识别低优先级微服务（如报表、测试任务），把它们迁移到普通节点或延迟调度，释放 GPU。

临时扩容：在资源池压力持续的情况下，协调客户使用 Spot 节点或跨 Region GPU，作为短期补充方案。

通过这些措施，客户的支付和风控服务在 GPU 资源紧张时仍能保持 99.9% 的 SLA；同时非关键任务通过调度延迟和 Spot 节点运行，整体业务连续性没有受到影响。最终，客户的 关键业务稳定性提升，而整体资源利用率也从 ~50% 提升到了 ~70%。

## 7. 请讲一次你推动跨部门合作解决问题的经历。

场景：大项目落地，需要协调 产品、销售、交付、供应链。

你的角色：资源协调者。

做法：召开跨部门 war room，推动需求确认、责任分工。

成果：方案顺利交付。

## 8. 有没有失败的案例？你学到了什么？

坦诚一个挑战：预测不足/资源调度慢。

后果：影响交付/客户不满。

学到的经验：要建立 预警机制、客户沟通机制。

📌 情境型问题

## 9. 如果市场上 AI 需求突然暴涨 3 倍，你会如何调整资源策略？

短期：抢占供应链资源，优先高价值客户。

中期：调度优化，引入分时复用。

长期：建立预测模型，推动供应链提前备货。

强调：既保障大客户 SLA，又兼顾整体收入。

## 10. 如果销售承诺的资源交付超出了当前供给，你会怎么处理？
👉 回答思路：

- 优先保证客户体验，但要短期应对（客户沟通 + 紧急兜底）透明沟通：第一时间和客户沟通真实情况，说明当前资源池的限制。

替代方案：提出阶段性交付（先满足部分，后续逐步补齐），或提供不同型号/不同地域 GPU 作为临时替代。

缓解措施：如果客户业务紧急，可以调度其他客户的闲置资源，或者调用抢占式 GPU 实例。

- 中期措施（内部协调）供应链加急：推动资源运营和采购团队，加急申请新 GPU 资源。

跨区域/多产品调用：协调其他 Region 的 GPU 库存，或者提供异构算力（CPU+GPU 混合、FPGA）。

资源优先级机制：保证战略客户和重点合同优先，避免 SLA 违约。

- 长期改进（机制建设）销售承诺与资源联动机制：推动在销售签大单前，必须走 资源预审流程，避免“先卖后找”的情况。

大客户资源预留池：对重点客户实行预留策略，把资源锁定在合同期内。

预测模型优化：把这类突发需求数据纳入预测，提升准确性。


## 11. 假设你现在是资源策略负责人，要给公司定一个 GPU 资源三年规划，你会怎么做？
👉 回答思路：

如果我是 GPU 资源策略负责人，我会从市场趋势、需求预测、供给策略、成本控制和风险管理五个方面来制定三年规划。

首先，市场趋势：未来三年 AI、大模型、视频渲染是 GPU 消耗的主要驱动力，供应链紧张，因此必须提前锁定资源。

在需求预测上，我会结合历史使用曲线和客户 pipeline，对 GPU 需求做时间序列预测，并设置 20% 的冗余 buffer。

在资源供给策略上，采用“长期自建 + 短期租赁 + 分时共享”的组合：稳定需求用自建 GPU 节点池，突发需求用合作伙伴和抢占式 GPU，应对不同客户场景。调度层面会引入 Kubernetes + Volcano，支持 GPU 分时共享和多租户隔离，提高利用率。

在成本与 ROI 上，我会设定 GPU 利用率 KPI，从 60% 提升到 80%，并通过分时计费、FinOps 成本分摊来降低闲置风险。

风险控制方面，会与多家厂商签订长期供货协议，建立大客户资源预留池，并保持架构的灵活性以适应 GPU 型号迭代。

三年目标是：第一年保障交付并提升利用率到 60%；第二年引入分时共享和异构算力，提升到 70%；第三年实现跨 Region GPU 池统一调度，利用率 80%，具备支撑超大规模 AI 训练的能力。

## 12. 如果资源利用率一直低下，怎么向管理层解释并提出改进方案？
👉 回答思路：

📌 一、如何向管理层解释（Why 利用率低）

你需要从 数据维度 + 业务维度 分析，不要只说“闲置”。

- 1. 数据维度

利用率统计：当前 GPU 利用率仅 40%–50%，低于行业基准（70%+）。

闲置分布：某些 Region / 某些型号 GPU 利用率低，导致整体偏低。

峰谷差：高峰期满载，非高峰期闲置 → 说明资源弹性不足。

- 2. 业务维度

供需错配：资源采购与客户需求不匹配（如 A100 买太多，客户实际用 T4/M40）。

客户预留未使用：签约大客户锁定资源，但实际消耗不足。

调度效率低：任务调度不均衡，有的池子爆满，有的长期空闲。

📌 二、提出的改进方案（How 提升利用率）
- 1. 短期措施（快速提升）

回收闲置资源：设置 Idle Timeout，对长时间未使用的 GPU 回收。

内部调度优化：跨 Region 调度，把空闲区的 GPU 调给紧张区客户。

推广抢占式 GPU：引导客户低价使用闲置 GPU（Spot 模式）。

- 2. 中期优化（3–6 个月）

GPU 分时共享（MIG / Time-slicing）：提升小任务场景下的碎片利用率。

调度系统优化：引入 Kubernetes + Volcano 等智能调度器，实现任务按需分配。

客户分级管理：对大客户实行按需预留+动态调整，避免“锁多用少”。

- 3. 长期机制（1–3 年）

供需预测体系：用时间序列模型（Prophet/ARIMA）+ 业务 Pipeline 来预测 GPU 需求，避免盲目采购。

异构算力布局：部分推理任务迁移到 CPU/FPGA/ASIC，降低 GPU 压力。

FinOps 成本透明：按客户/业务线分摊 GPU 成本，推动部门节省资源。


## 第 3 条：负责火山引擎对外资源策略的制定，配合资源运营达成运营指标

🔹 关键词解释

对外资源策略：指火山引擎面向客户/市场的资源供给策略，比如如何分配 GPU、CPU、存储、带宽等资源，确保客户的 SLA 和交付。

资源运营指标：通常包括资源利用率、成本控制率、供需匹配度、毛利率、SLA 达成率等。

🔹 理解
这条的核心是：你要制定 资源供给和分配的策略，比如：

如何保证大客户优先（战略客户、AI 公司）？

如何定价 Spot/抢占式 GPU？

不同 Region 的资源如何配比，避免过度闲置？
然后要配合运营团队，把指标落实（比如 GPU 利用率提升到 70%、整体成本下降 15%）。


## 第 4 条：负责火山引擎新资源外部需求输入，配合相关团队在市场侧完成落地

🔹 关键词解释

新资源外部需求输入：指市场/客户提出的新需求，例如：

某客户要求 H100 GPU 支持，当前资源池没有 → 需要你收集、评估、反馈。

某行业需要低时延存储、RDMA 网络、超大内存规格 → 需要你把需求拉通到产品/研发。

市场侧落地：把需求落实成可售卖的产品或服务，并且交付给客户。

🔹 理解
这条的核心是：你是 需求传递与落地的桥梁，要做两件事：

收集外部需求（客户、行业、销售反馈）。

推动内部团队落地（产品、研发、运营），确保市场上能真正交付。

## 不同 Region 的资源如何配比？

热点 Region：重点投放主流 GPU 型号（如 A100/H100），保证大客户交付。

冷门 Region：投放次一级 GPU（如 T4、V100），承接测试/推理类任务。

跨区调度：允许部分任务通过网络/调度迁移到 GPU 更充足的 Region。
👉 面试思路：先看需求侧（客户在哪），再看供给侧（机房/供应链能给什么），最后做冷热搭配，避免一边紧张一边闲置。
## 📌 GPU 有哪些型号？怎么考虑分配？

常见 GPU 型号分层：训练型（高算力，适合大模型）：NVIDIA H100 / A100：旗舰，主要用于大模型训练、科研计算；V100：上一代训练卡，部分客户仍在用。

推理型（低成本，适合部署和实时业务）：T4：主流推理卡，能效比高，成本低，常用于推荐、视频转码；L4：新一代推理卡，替代 T4 的趋势。

通用/边缘型：A10 / A30：兼顾训练 + 推理，灵活性高；消费级 RTX 3090/4090：部分云厂商也会提供，面向中小客户。

分配思路：AI 训练客户：优先分配 A100/H100，确保 SLA；大规模推理（推荐、搜索、广告）：投放 T4/L4，提升利用率；混合需求（中小客户、科研）：A10/V100 作为补充；长尾需求：可用消费级 GPU 池支撑。

## 📌 三、这个岗位需要写需求分析吗？包含哪些内容？

是的，这个岗位一定需要写需求分析/方案，内容通常包括：

- 需求来源（谁提的需求，背景是什么）来自客户？市场？销售？还是内部运营？

- 需求描述（要什么资源，在哪个 Region，用途是什么）比如：客户 X 要 500 块 H100，华北 Region，用于大模型训练，要求 3 个月内交付。

- 需求优先级 战略客户、大客户、长期合同 > 中小客户、试验性需求。

- 资源供给分析：现有库存多少？交付周期多久？Region 是否支持？

- 风险与约束：如果满足不了，会不会影响签单？成本/利用率风险有多大？

- 决策与行动计划：是否采购？是否跨区调度？是否用替代型号（A100 → A10 + T4 混合）？

- 在遇到多个客户同时有关键任务时，我会写一份资源分配项目计划书。
首先明确背景和目标：说明当前 GPU 资源不足，需要在保证 SLA 的前提下，合理分配给不同客户。
接着对客户需求做表格化分析，列清楚每个客户的资源需求、时间窗口和优先级。
然后制定资源供给方案，比如 A100/H100 优先给战略客户，大规模推理任务用 T4/L4 补充；同时规划时间表和调度策略，确保任务不冲突。
最后加入风险评估和应急方案，比如供应链交付延迟时是否跨区调度或临时租赁 GPU。
这样一份计划书可以保证资源分配透明、可追溯，同时让业务和运营团队有一致预期。


## 📌 一、GPU 资源供给的典型方案
- 1. 专属型（Dedicated GPU）

每个客户/任务独占整块 GPU（如 A100/H100）。

优点：性能稳定，适合 大模型训练、科学计算。

缺点：利用率低，小任务容易浪费。

- 2. 共享型（Shared GPU / 分时/分区）

一块 GPU 被多个任务共享：时间片共享（Time-Slicing）：不同任务轮流使用 GPU。MIG（Multi-Instance GPU）：NVIDIA A100/H100 支持硬件切分，把 1 块卡分成 7 个小实例。vGPU（Virtual GPU）：虚拟化层抽象成多个逻辑 GPU。

优点：提高利用率，适合 推理、小规模训练。

缺点：需要额外调度和隔离机制。

- 3. 抢占式/竞价型 GPU（Preemptible / Spot GPU）

类似 Spot VM，价格低，但可能随时被回收。

优点：成本低，适合 可容忍中断的任务（批处理训练、测试）。

缺点：不适合关键任务。

- 4. 多层池化方案

将 GPU 分成 基础池（低成本推理）、高性能池（大模型训练）、抢占池（低价批处理）。按需调度，不同客户进入不同的资源池。

优点：灵活，兼顾不同客户需求。

📌 二、GPU 资源的运营优化方案
- 1. 调度层面

Kubernetes + Volcano / KubeFlow：支持 Job 级调度、优先级队列。Gang Scheduling：一组任务要么全分配到 GPU，要么等待，避免浪费。跨 Region 调度：把任务迁移到 GPU 更充足的区域。

- 2. 成本与利用率优化

分时共享/MIG → 提高 GPU 使用颗粒度。Idle Timeout 回收 → 长期低利用率自动释放。异构算力替代 → 推理任务下沉到 T4/L4，训练任务保留 H100/A100。

- 3. 客户/业务策略

大客户预留池：战略客户提前锁定 GPU。FinOps 成本分摊：按业务线/客户结算 GPU 成本，推动节省。差异化 SLA：关键业务分配高性能 GPU，普通业务用抢占式。

📌 三、GPU 资源的交付与弹性方案
- 1. 弹性伸缩

结合 K8s Cluster Autoscaler，按需拉起 GPU 节点。峰值期间临时扩容，闲时缩容。

- 2. 混合云/多云 GPU

在自有 GPU 不足时，短期租用 AWS/GCP/Azure GPU。通过 统一调度平台（如 Ray、Slurm、K8s）整合外部算力。

- 3. 容灾与冗余

GPU 工作负载跨 Region 复制，避免单区域 GPU 短缺影响 SLA。关键任务绑定主备 GPU 池。

📌 四、面试时的回答示例

GPU 资源方案一般分为三类：
供给方式上，有专属 GPU、共享 GPU（MIG/vGPU/分时）、抢占式 GPU；
运营优化上，会用调度器做优先级管理，通过分时共享和 Idle 回收提升利用率，并结合大客户预留和 FinOps 控制成本；
交付方式上，可以用弹性伸缩、跨 Region 调度以及混合云 GPU 补充来保障 SLA。

对训练型客户，我会优先分配 A100/H100；对推理型客户，则以 T4/L4 为主；对低优先任务，推荐抢占式 GPU。这种分层次的方案既能保障关键客户体验，也能提升整体 ROI。

## 一、Spot/抢占式 GPU 的定价机制

按剩余资源市场价浮动：云厂商（Azure、AWS、GCP）会把 数据中心的闲置 GPU 资源 以低价出售。价格通常比按需实例（On-Demand）便宜 60%~90%。一旦资源紧张，实例可能会被 回收/中断（通常有 30 秒~2 分钟通知）。

不同厂商模式：Azure Spot VM (GPU)：用户选择 VM 优先级（Spot），可设定 最高愿付价格 或接受随市场波动；若被回收，可以自动转为队列/重试。AWS EC2 Spot GPU：用户出价机制已简化，价格随供需浮动；支持 Spot Fleet/Spot Block（指定时长）。GCP Preemptible GPU：价格固定（通常比 On-Demand 便宜 70%），但最长只能运行 24 小时，随时可能被回收。

二、如何给 Spot GPU 定价（企业策略）

基准价参考：先查目标区域 On-Demand GPU 的单价（比如 Azure East US A100 ~$3.5/h）。Spot GPU 价格一般是 On-Demand 的 20%~40%。可以用 历史价格曲线（Azure Advisor/AWS Spot Advisor/GCP Billing API） 做参考。

设定 Max Price（Azure/AWS）：建议设置为 不超过 On-Demand 50%~60%，否则失去性价比。比如 A100 On-Demand = $3.5/h → Spot Max Price 可设 ~$2/h。

混合定价策略：核心训练任务：固定 On-Demand 或 Reserved GPU，保证稳定。实验/批处理/可中断训练：使用 Spot GPU，降低成本。策略组合：70% On-Demand + 30% Spot，或采用 K8s 的 PriorityClass + NodePool 混合调度。

三、如何利用 Spot GPU 控制成本

容错任务：批处理、超参数搜索（Hyperparameter Tuning）、模型并行训练的子任务。即使 Spot 被回收，也不会导致整体训练失败。

分布式调度：使用 Kubernetes + Kubeflow/Argo，把任务切成小块，Spot 回收后能自动重试。检查点机制（Checkpointing）：比如 TensorFlow tf.train.Checkpoint，PyTorch torch.save。

容量规划：通过 Prometheus/Grafana 监控 Spot 中断率（Azure CLI/AWS API 提供历史数据）。结合 AutoScaler：资源紧张时自动回落到 On-Demand。

四、一个实际例子

场景：NLP 模型预训练，单卡 40GB A100，需求 10 台 GPU，预期训练 1 周。

On-Demand 成本：$3.5/h × 10 × 168h ≈ $58,800。

Spot 策略：70% On-Demand（7 台）+ 30% Spot（3 台，50% 折扣）。

Spot 成本：$1.75/h × 3 × 168h ≈ $8,820。

总成本：$58,800 – (3 台 On-Demand $17,640 – Spot $8,820) ≈ $49,980。
➡ 直接节省约 15%，且不会因为 Spot 中断导致训练失败。

## 一般对IAAS的监控有哪些方面的指标
IaaS 监控常看六大类：

- 计算（Compute）

CPU：利用率、负载、throttling/steal。内存：使用率、swap、OOM 次数。磁盘（本地/数据盘）：IOPS、吞吐、平均/尾延迟、队列深度。可用性：VM/主机健康、重启/崩溃事件、自动伸缩事件。GPU（如有）：SM 利用率、显存占用、ECC 错误、驱动健康。

- 存储（Storage）

块存储：容量利用率、IOPS/吞吐/延迟、突发余额、快照/备份成功率。对象存储：请求量、4xx/5xx、平均/95P 延迟、数据出站。文件存储：IOPS/吞吐、挂载错误、锁竞争。

- 网络（Networking）

带宽与 PPS：ingress/egress、连接数。质量：时延、抖动、丢包、重传。设备/网关：NAT 端口耗尽率、路由收敛、DNS 成功率。负载均衡：健康探测通过率、后端 5xx、连接数、请求时延。

- 安全与合规（Security）

安全组/WAF 命中、阻断次数、异常流量。漏洞与补丁：基线合规率、未修复 CVE。身份与权限：IAM 变更、失败的鉴权/授权。

- 可用性与韧性（SLA/Resilience）

多 AZ/机组分布、故障域均衡。维护事件影响面、故障切换时间（MTTR）、无故障时间（MTBF）。备份/恢复：RPO/RTO 达标率，演练结果。

- 成本与配额（Cost/Quota）

资源利用率 vs 预留/包年（RI/Reserved）覆盖。配额使用率（vCPU、IP、磁盘数）、超配风险。Spot：回收率、节省率、可得性趋势。
抢占式实例 / 分时复用：把一些非关键任务（测试、低优先级训练）迁出，释放 GPU。

## 监控k8s有什么指标
- 节点层（Node Metrics）

CPU 使用率 / 饱和度（Usage, Throttling），内存使用率 / OOM Kill 次数，磁盘 I/O 吞吐量、延迟，网络带宽（In/Out）

- Pod / 容器层（Pod & Container Metrics）

Pod CPU / 内存请求与实际使用，Pod 重启次数（CrashLoopBackOff），容器状态（Running、Waiting、Terminated），容器文件系统使用情况

- 集群资源层（Cluster Metrics）

节点可用数 / NotReady 节点数，调度失败次数（Pending Pod 数量），资源分配率（Requests / Limits vs Capacity），API Server QPS、请求延迟、错误率

- 服务与应用层（Service / Workload Metrics）

Deployment / ReplicaSet 的副本数与可用率，Service 请求延迟、错误率（通常配合 Service Mesh/Ingress），etcd 存储大小、操作延迟（影响控制平面稳定性），控制器（Scheduler、Controller Manager）健康状态

- 节点层：CPU/内存/磁盘/网络异常 → 导致 节点不可用，业务 Pod 无法运行，整体吞吐下降。

- Pod/容器层：资源不足或频繁重启 → 造成 应用不稳定，请求失败率上升，用户体验受影响。

- 集群层：Pending Pod、API Server 延迟高 → 出现 调度阻塞，新任务无法及时上线，影响交付效率。

- 服务层：副本数不足、请求延迟/错误率升高 → 直接导致 应用性能下降或宕机，客户感知最明显。

## 在 Azure 上做资源运营（要点打法）

- 可观测性：用 Azure Monitor/Log Analytics 建 SLI（延迟/错误率/吞吐），按业务维度打 Tag 聚合。

- 容量与弹性：基于历史指标做 预测；用 VMSS Autoscale/AKS Cluster Autoscaler；关键工作负载配 Capacity Reservation，并管理 Quota。

- 供给与成本：按 On-Demand + Reserved/Savings Plan + Spot 分层；结合 Advisor 做 Rightsizing；用 Cost Management + Budgets 控费。

- 可靠性：使用 Availability Zones、健康探测与 LB、Backup/ASR，设 SLO/错误预算 与扩缩容策略。

- 安全与合规：Defender for Cloud、Azure Policy、RBAC/MI 持续基线治理。

- 变更与自动化：Bicep/Terraform 管理 IaC；Update Manager、Action Group + Logic Apps/Functions 做告警自愈与定时启停。

## GPU出现问题如何修复和资源分配

- 驱动/CUDA 不匹配：版本漂移导致 nvidia-smi 报错、框架加载失败。

修复：用 NVIDIA Driver Extension 重装匹配版本；统一 CUDA/cuDNN。

策略：建立 版本矩阵 与 Golden Image/Shared Image Gallery，容器基镜像 版本固化（pin），灰度环（canary）发布，Kernel hold 避免意外升级。

- 内核更新 / Secure Boot 拒载（DKMS 失败）：dmesg 出现 module verification failed。

修复：安装匹配 kernel-headers、重建 DKMS；需要时签名驱动或调整 Secure Boot。

策略：用 Update Manager/维护窗口 管控补丁，先 预生产回归，设置 自动回滚 与告警触发。

- 容器栈缺件（nvidia-container-toolkit）：容器内看不到 GPU。

修复：安装 toolkit，daemon.json 设 default-runtime=nvidia；重启/重建。

策略：AKS 建 基线 Addon（Driver DS+Device Plugin），用 OPA/Gatekeeper/Azure Policy 做准入校验，CI 做 镜像健康检查。

- 配额/区域容量不足：创建失败或 Pod 长期 Pending。

修复：提 Quota、换区/换规格、准备替代规格映射。

策略：做 容量预测 与 提前申请，关键业务用 Capacity Reservation；供给侧用 按需+预留/节省计划+Spot 分层，启用 多区域/多规格回退。

- 硬件健康/ECC/Xid 错误：nvidia-smi -q/dmesg 告警。

修复：先 Redeploy VM 迁走宿主；持续复现走工单/RMA。

策略：多 AZ/故障域均衡、N+1 余量、健康阈值 触发 替换/排空 Runbook。

- 热/功耗限流（Thermal/Power Throttle）：P-State 频繁降档，吞吐掉。

修复：降低并发/批量大小，分时调度；必要时换 更高散热/带宽 SKU。

策略：为训练队列设 功耗/并发上限，用 作业编排+预算队列 平滑峰值。

- MIG/vGPU 配置冲突（A100/A30）：Profile 不一致导致调度失败/显存不足。

修复：统一 MIG Profile，按 Profile 建 独立节点池；调度声明精确资源。

策略：池级标准化（labels/taints），队列侧做 Profile 感知 与回退策略。

- AKS 设备插件/驱动缺失：nvidia.com/gpu 不可分配。

修复：部署 NVIDIA Device Plugin/Driver DS，校验节点 allocatable。


## 云上/自建资源池如何分层与池化（计算/存储/网络/GPU）
框架：4×4 分层法

- 按业务层级：在线（延迟敏感）/ 实时推理 / 离线批 / 训练实验。 

- 按资源类型：计算（CPU/内存）在线和离线分池、可中断单独弹性池，有状态隔离池；在线优先级最高/ 存储（盘/对象/元数据）把低延迟小 I/O放在热盘/高 IOPS，顺序吞吐走对象/并行文件；日志/数据/系统分盘/ 网络（南北向/东西向）/ GPU（推理/训练）。推理走 MIG/MPS 保密度与 QoS，训练走整卡保吞吐

- 按可用性域：多机房/多可用区/故障域（机架/机箱）。

- 按成本属性：基座（预留/自建）/ 弹性（按需）/ 降本（Spot/可中断）/ 专用（合规/隔离）。

## 多租户隔离与混部安全如何同时保障
- 硬隔离（必须有）：配额与层级配额：Org→Team→Project 的 CPU/内存/GPU/存储/带宽配额。

节点污点/容忍/亲和：关键租户或有状态工作负载专属节点池。

MIG/整卡绑定：GPU 租户 QoS。

网络策略：默认拒绝的 PodNetworkPolicy，仅允许白名单流量。

存储隔离：不同租户独立卷与 KMS 加密；快照/克隆权限边界。

- 软隔离（强烈建议）：QoS 类别：Guaranteed/Burstable/BestEffort 映射到调度权重与资源回收。

限流与熔断：入口限 QPS、出站带宽/pps 限制，避免“吵闹邻居”。

cgroup 上限：CPU 上限、内存上限+OOM 策略、I/O 带宽/IOPS 上限。

- 准入&合规（防御前移）：OPA/Gatekeeper/准入 Webhook：强制镜像签名、资源上限、特权容器禁用、敏感挂载禁止。

镜像供应链：私有镜像仓库、签名（Cosign）、高危基线扫描（Trivy/Clair）。

- 调度与公平：优先级与抢占：SLA→PriorityClass，抢占只针对可中断工作负载，设置冷却时间与上限。

公平算法：同优先级内用 DRF；离线队列支持 Backfill。

反亲和/拓扑：热点打散，防热点/防抖动。

## 安全库存（Safety Stock）怎么定
- 思路：三块风险合并

需求侧不确定性：预测误差（季节性/活动/突发）。

供给侧不确定性：交付前置期（上架/采购/配额）、故障/维护窗口。

SLO 等级：在线 > 推理 > 离线/训练。

- 两种常用定法

分位数法（推荐）：按 P95/P99 流量或资源需求做冗余。

在线：P95 + 5–15%（大促前调到 +20–30%）。

推理：P95 + 10–20%（看冷启动成本）。

离线/训练：按排队时延目标推导（见下）。
<img width="1284" height="883" alt="image" src="https://github.com/user-attachments/assets/1fea9db1-5348-4b67-811b-3bf3e845f254" />


## 容量规划指标与目标利用率（CPU/内存/GPU/带宽/IOPS）
- 规划流程（5 步）

1. 分桶：按业务线/集群/AZ/机型/资源类型。

2. 预测：趋势+季节性+事件修正，给出 P50/P90/P95。

3. SLO→资源：把延迟/丢包/错误率目标映射为目标利用率与排队上限。

4. 冗余：安全库存 + 故障域冗余（N+1、跨 AZ）。

5. 采购与弹性：确定**基座（预留/自建）与弹性（按需/Spot）**比例。


## 峰值保供与大促演练怎么做？

- 前置：容量压测（生产影子流量/合成流量），预热镜像与数据，跨 AZ 扩容，开「只增不减」策略。

- 中场：QPS 限流、熔断降级、缓存加大、静态化页面、灰度与快速回退。

- 后置：复盘峰值利用率/告警差距，修正预测模型与安全库存。

## DRF（Dominant Resource Fairness）是什么？和优先级抢占如何配合？

- 概念：在多维资源（CPU/内存/GPU/IO）下，不同作业的瓶颈不一样。
DRF 会看每个作业在“自己最稀缺的那类资源”上的占比，尽量让所有作业的主导资源占比相等，这样公平。

- 例子：JobA 主要吃 CPU，JobB 主要吃内存。DRF 会保证 A 在 CPU 维度和 B 在内存维度的“最大份额”相对均衡。

- 和优先级抢占结合：先看 优先级（SLA 等级/付费等级），优先级低的不能抢高的。

在同一优先级内，再按 DRF 做多维公平分配。

如果资源紧张，启用 抢占机制：只抢可中断任务（Spot/批处理），控制抢占频率，避免抖动。

- 面试话术：
“我会先用优先级保障 SLA，然后在同级内用 DRF 解决 CPU/内存/GPU 的公平性。极端情况下才允许抢占，且只动低优先级或可中断任务。

## 批任务/AI 训练的 Gang Scheduling、Backfill 是什么？适用场景？

- Gang Scheduling（帮派调度）:要么全体任务一起启动，要么都不启动。

适用场景：需要多卡同步的训练、MPI 类分布式计算。

好处：避免只分到部分资源卡死。

- Backfill（回填调度）:大作业在排队，空出来的零碎资源可以临时插入短任务。

适用场景：批任务、离线作业，提升整体资源利用率。

好处：减少碎片浪费，同时不影响大作业的等待顺序。

- 面试话术：“分布式训练我会用 Gang Scheduling 保证同步启动；在有碎片资源时用 Backfill 插入短任务，提高集群利用率。”

## 预留、按需、Spot 如何组合？怎么算覆盖率与风险敞口？

- 三类资源模式：预留（Reserved/自建）：便宜但长期绑定，覆盖基线负载。按需（On-demand）：灵活但贵，处理不可预测的弹性需求。Spot/可中断：最便宜，承担可容错的批/训练任务，但有中断风险。

- 组合思路：

1.基线需求：预留池（保障长期稳定负载）。

2.波动/增长：按需池（保障 SLA）。

3.弹性降本：Spot（批处理/可中断 AI 训练）。

- 覆盖率（Coverage）：用预留/稳定资源占比 ÷ 总需求衡量。

例如 P95 需求是 1000 核，我有 700 核预留，覆盖率 70%。

- 风险敞口：依赖 Spot 的部分算风险敞口。

如果业务 300 核跑在 Spot 上，就有 30% 的敞口，需要 fallback 到按需或降级方案。

- 面试话术:“我会先用预留覆盖 60–70% 的基线，按需兜底 SLA，Spot 控制在 20–30%，并设回落通道计算风险敞口。”

## 不同 SLA 等级如何落到资源冗余与调度优先级？

- SLA 高（金融/支付/核心交易）:多 AZ 容灾、双活或热备，资源冗余 100%。调度优先级最高，保证不被抢占。

- SLA 中（一般在线服务）:单 AZ 冗余 + 异地冷备，冗余率 30–50%。调度优先级高，但允许少量抢占或降级。

- SLA 低（内部测试/批处理）:无额外冗余，允许跑在 Spot/低优先级节点。可被高等级抢占。
策略：节点就绪门槛（Readiness Gate）、预热池（Warm Pool）、滚动升级设 maxSurge/PDB，失败自动回滚。
